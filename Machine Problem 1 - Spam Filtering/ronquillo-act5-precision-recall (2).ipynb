{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10961292,"sourceType":"datasetVersion","datasetId":6819417},{"sourceId":11094979,"sourceType":"datasetVersion","datasetId":6916307}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Precision and Recall\n### by: Kyla S. Ronquillo\n\n1. Write a function that computes the precision and recall measures.\n2. Check the formula below for the Precision and Recall.\n3. Check the LabeledTestData.csv for the correct labels.\n4.  Compare it to your SurnameResultData.csv\n5.  Output a file SurnamePrecisionRecall.csv, same as your that contains new columns at the beginning: measure (values either TP, TN, FP, FN) and correct_label (from the LabeledTestData.csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:23:13.368273Z","iopub.execute_input":"2025-03-20T00:23:13.368635Z","iopub.status.idle":"2025-03-20T00:23:13.373186Z","shell.execute_reply.started":"2025-03-20T00:23:13.368604Z","shell.execute_reply":"2025-03-20T00:23:13.371760Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Define file paths\nLabeledDataSet = \"/kaggle/input/dataset-of-messages/LabeledTestData.csv\"\nResultData_NB = \"/kaggle/input/new-results/RonquilloResultData.csv\"\nResultData_RF = \"/kaggle/input/new-results/random_forest_resultss.csv\"\n\n#-----------------------------\ndf = pd.read_csv(ResultData_RF)\n\n# Rename the column\ndf.rename(columns={'predicted_label': 'label'}, inplace=True)\n\n# Save the updated DataFrame in a writable location\nupdated_file_path = \"/kaggle/working/random_forest_results_updated.csv\"\ndf.to_csv(updated_file_path, index=False)\n\n# Update the variable to point to the new file\nResultData_RF = updated_file_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:23:13.375631Z","iopub.execute_input":"2025-03-20T00:23:13.376014Z","iopub.status.idle":"2025-03-20T00:23:13.416185Z","shell.execute_reply.started":"2025-03-20T00:23:13.375978Z","shell.execute_reply":"2025-03-20T00:23:13.414945Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def compute_precision_recall(labeled_file, result_file, output_file):\n    labeled_df = pd.read_csv(labeled_file, encoding='ISO-8859-1')\n    result_df = pd.read_csv(result_file, encoding='ISO-8859-1')\n    \n    print(\"Labeled Data Columns:\", labeled_df.columns)\n    print(\"Result Data Columns:\", result_df.columns)\n    \n    if 'label' not in labeled_df.columns or 'label' not in result_df.columns:\n        raise ValueError(f\"Expected column 'label' not found.\\n\"\n                         f\"Labeled Data Columns: {labeled_df.columns}\\n\"\n                         f\"Result Data Columns: {result_df.columns}\")\n    \n    result_df.rename(columns={'label': 'predicted'}, inplace=True)\n    merged_df = labeled_df.copy()\n    merged_df['predicted'] = result_df['predicted']\n    \n    conditions = [\n        (merged_df['label'] == 'spam') & (merged_df['predicted'] == 'spam'),\n        (merged_df['label'] == 'ham') & (merged_df['predicted'] == 'ham'),\n        (merged_df['label'] == 'ham') & (merged_df['predicted'] == 'spam'),\n        (merged_df['label'] == 'spam') & (merged_df['predicted'] == 'ham')\n    ]\n    values = ['TP', 'TN', 'FP', 'FN']\n    \n    merged_df['measure'] = np.select(conditions, values, default='Unknown')\n    \n    TP = (merged_df['measure'] == 'TP').sum()\n    FP = (merged_df['measure'] == 'FP').sum()\n    FN = (merged_df['measure'] == 'FN').sum()\n    \n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    \n    merged_df.rename(columns={'label': 'correct_label'}, inplace=True)\n    merged_df[['measure', 'correct_label', 'predicted']].to_csv(output_file, index=False)\n    print(f\"Output saved to {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:23:13.417289Z","iopub.execute_input":"2025-03-20T00:23:13.417647Z","iopub.status.idle":"2025-03-20T00:23:13.430633Z","shell.execute_reply.started":"2025-03-20T00:23:13.417610Z","shell.execute_reply":"2025-03-20T00:23:13.428137Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Execute the function with predefined file paths\ncompute_precision_recall(LabeledDataSet, ResultData_NB, \"RonquilloPrecisionRecall_NB.csv\")\ncompute_precision_recall(LabeledDataSet, ResultData_RF, \"RonquilloPrecisionRecall_RF.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:23:13.432508Z","iopub.execute_input":"2025-03-20T00:23:13.432870Z","iopub.status.idle":"2025-03-20T00:23:13.502182Z","shell.execute_reply.started":"2025-03-20T00:23:13.432835Z","shell.execute_reply":"2025-03-20T00:23:13.501067Z"}},"outputs":[{"name":"stdout","text":"Labeled Data Columns: Index(['label', 'message'], dtype='object')\nResult Data Columns: Index(['message', 'label'], dtype='object')\nPrecision: 0.8333\nRecall: 0.9430\nOutput saved to RonquilloPrecisionRecall_NB.csv\nLabeled Data Columns: Index(['label', 'message'], dtype='object')\nResult Data Columns: Index(['message', 'actual_label', 'label'], dtype='object')\nPrecision: 1.0000\nRecall: 0.7895\nOutput saved to RonquilloPrecisionRecall_RF.csv\n","output_type":"stream"}],"execution_count":25}]}