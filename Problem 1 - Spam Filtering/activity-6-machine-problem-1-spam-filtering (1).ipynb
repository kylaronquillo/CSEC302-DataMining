{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d70ab9b",
   "metadata": {
    "papermill": {
     "duration": 0.00513,
     "end_time": "2025-03-08T12:55:17.602504",
     "exception": false,
     "start_time": "2025-03-08T12:55:17.597374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Naive Bayes  and Random Forest Spam Filter\n",
    "### by: Kyla S. Ronquillo\n",
    "\n",
    "\n",
    "This notebook contains a from-the-scratch implementation of Spam/Ham filter using the Naive Bayes theory. Meanwhile, my implementation of the model Random Forest was done with the help of scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deafc2c1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:17.612579Z",
     "iopub.status.busy": "2025-03-08T12:55:17.612129Z",
     "iopub.status.idle": "2025-03-08T12:55:18.644193Z",
     "shell.execute_reply": "2025-03-08T12:55:18.643159Z"
    },
    "papermill": {
     "duration": 1.039121,
     "end_time": "2025-03-08T12:55:18.646125",
     "exception": false,
     "start_time": "2025-03-08T12:55:17.607004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c043ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:18.656948Z",
     "iopub.status.busy": "2025-03-08T12:55:18.656452Z",
     "iopub.status.idle": "2025-03-08T12:55:20.711514Z",
     "shell.execute_reply": "2025-03-08T12:55:20.710004Z"
    },
    "papermill": {
     "duration": 2.063094,
     "end_time": "2025-03-08T12:55:20.714098",
     "exception": false,
     "start_time": "2025-03-08T12:55:18.651004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# libraries for random forest \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2a7a2",
   "metadata": {
    "papermill": {
     "duration": 0.004362,
     "end_time": "2025-03-08T12:55:20.724738",
     "exception": false,
     "start_time": "2025-03-08T12:55:20.720376",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess the Training Data\n",
    "\n",
    "1. Read TrainingData.csv (contains 3900 messages labeled as \"ham\" or \"spam\").\n",
    "2. Remove special characters and tokenize words (keep only alphabetic characters).\n",
    "3. Create a vocabulary (V) of unique words.\n",
    "4. Count the occurrences of each word in ham and spam messages.\n",
    "5. Compute the prior probabilities for ham and spam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd2ac8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:20.737536Z",
     "iopub.status.busy": "2025-03-08T12:55:20.736832Z",
     "iopub.status.idle": "2025-03-08T12:55:20.796598Z",
     "shell.execute_reply": "2025-03-08T12:55:20.795544Z"
    },
    "papermill": {
     "duration": 0.067649,
     "end_time": "2025-03-08T12:55:20.798307",
     "exception": false,
     "start_time": "2025-03-08T12:55:20.730658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the training data\n",
    "\n",
    "df_sms = pd.read_csv('/kaggle/input/messages-data/TrainingData.csv', encoding='ISO-8859-1')\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353dc1f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:20.808664Z",
     "iopub.status.busy": "2025-03-08T12:55:20.808327Z",
     "iopub.status.idle": "2025-03-08T12:55:20.839155Z",
     "shell.execute_reply": "2025-03-08T12:55:20.838033Z"
    },
    "papermill": {
     "duration": 0.037909,
     "end_time": "2025-03-08T12:55:20.840764",
     "exception": false,
     "start_time": "2025-03-08T12:55:20.802855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clean text and tokenize words\n",
    "\n",
    "def preprocess_data(text):\n",
    "    #keep alphabet and space only\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    #lowercase all of them\n",
    "    text = text.lower()\n",
    "    #split into words\n",
    "    words = text.split()\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "df_sms['tokens'] = df_sms['message'].apply(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c6ced83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:20.850949Z",
     "iopub.status.busy": "2025-03-08T12:55:20.850574Z",
     "iopub.status.idle": "2025-03-08T12:55:20.868893Z",
     "shell.execute_reply": "2025-03-08T12:55:20.867903Z"
    },
    "papermill": {
     "duration": 0.025117,
     "end_time": "2025-03-08T12:55:20.870441",
     "exception": false,
     "start_time": "2025-03-08T12:55:20.845324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7063\n"
     ]
    }
   ],
   "source": [
    "#V = vocabulary of unique words\n",
    "\n",
    "all_words = []\n",
    "for tokens in df_sms['tokens']:\n",
    "    for word in tokens:\n",
    "        all_words.append(word)\n",
    "\n",
    "V = set(all_words)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(V)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01eb821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:20.886633Z",
     "iopub.status.busy": "2025-03-08T12:55:20.886062Z",
     "iopub.status.idle": "2025-03-08T12:55:20.906669Z",
     "shell.execute_reply": "2025-03-08T12:55:20.904810Z"
    },
    "papermill": {
     "duration": 0.033754,
     "end_time": "2025-03-08T12:55:20.909223",
     "exception": false,
     "start_time": "2025-03-08T12:55:20.875469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(ham) = 0.8669, P(spam) = 0.1331\n"
     ]
    }
   ],
   "source": [
    "#count their statistics and report the prior probabilities for spam and ham\n",
    "\n",
    "# Count the number of ham and spam messages and total messages\n",
    "total_sms = len(df_sms)\n",
    "num_spam = len(df_sms[df_sms['label'] == 'spam'])\n",
    "num_ham = len(df_sms[df_sms['label'] == 'ham'])\n",
    "\n",
    "probability_spam = num_spam/total_sms\n",
    "probability_ham = num_ham/total_sms\n",
    "\n",
    "print(f\"P(ham) = {probability_ham:.4f}, P(spam) = {probability_spam:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18cffb",
   "metadata": {
    "papermill": {
     "duration": 0.00806,
     "end_time": "2025-03-08T12:55:20.926736",
     "exception": false,
     "start_time": "2025-03-08T12:55:20.918676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the Naive Bayes Classifier\n",
    "\n",
    "1. Use Laplace Smoothing (Î» = 1) to compute the likelihoods.\n",
    "2. Store word probabilities for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b6f709d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:20.942151Z",
     "iopub.status.busy": "2025-03-08T12:55:20.941791Z",
     "iopub.status.idle": "2025-03-08T12:55:20.971269Z",
     "shell.execute_reply": "2025-03-08T12:55:20.970019Z"
    },
    "papermill": {
     "duration": 0.03902,
     "end_time": "2025-03-08T12:55:20.973270",
     "exception": false,
     "start_time": "2025-03-08T12:55:20.934250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#count word freqs. in Spam and Ham Messages\n",
    "from collections import defaultdict\n",
    "\n",
    "#separate messages by class\n",
    "ham_messages = df_sms[df_sms['label'] == 'ham']['tokens']\n",
    "spam_messages = df_sms[df_sms['label'] == 'spam']['tokens']\n",
    "\n",
    "#initialize word count dictionaries\n",
    "ham_word_counts = defaultdict(int)\n",
    "spam_word_counts = defaultdict(int)\n",
    "\n",
    "#count word occurrences in ham messages\n",
    "for tokens in ham_messages:\n",
    "    for word in tokens:\n",
    "        ham_word_counts[word] += 1\n",
    "\n",
    "#count word occurrences in spam messages\n",
    "for tokens in spam_messages:\n",
    "    for word in tokens:\n",
    "        spam_word_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f184fe5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:20.990051Z",
     "iopub.status.busy": "2025-03-08T12:55:20.989656Z",
     "iopub.status.idle": "2025-03-08T12:55:21.007017Z",
     "shell.execute_reply": "2025-03-08T12:55:21.005596Z"
    },
    "papermill": {
     "duration": 0.026534,
     "end_time": "2025-03-08T12:55:21.009109",
     "exception": false,
     "start_time": "2025-03-08T12:55:20.982575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vocabulary size (number of unique words)\n",
    "V_size = len(V)\n",
    "\n",
    "# Total word count in each class\n",
    "total_ham_words = sum(ham_word_counts.values())\n",
    "total_spam_words = sum(spam_word_counts.values())\n",
    "\n",
    "# Compute word probabilities with Laplace Smoothing\n",
    "ham_probabilities = {word: (ham_word_counts[word] + 1) / (total_ham_words + V_size) for word in V}\n",
    "spam_probabilities = {word: (spam_word_counts[word] + 1) / (total_spam_words + V_size) for word in V}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e140b2f3",
   "metadata": {
    "papermill": {
     "duration": 0.007897,
     "end_time": "2025-03-08T12:55:21.025258",
     "exception": false,
     "start_time": "2025-03-08T12:55:21.017361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Classify the Test Data\n",
    "\n",
    "1. Read TestData.csv (1672 messages without labels).\n",
    "2. For each message, compute the probability of being spam or ham using log probabilities.\n",
    "3. Assign the label ham or spam based on the higher probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b2ab48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:21.038922Z",
     "iopub.status.busy": "2025-03-08T12:55:21.038543Z",
     "iopub.status.idle": "2025-03-08T12:55:21.069864Z",
     "shell.execute_reply": "2025-03-08T12:55:21.068331Z"
    },
    "papermill": {
     "duration": 0.039867,
     "end_time": "2025-03-08T12:55:21.071867",
     "exception": false,
     "start_time": "2025-03-08T12:55:21.032000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  \\\n",
      "0  That depends. How would you like to be treated...   \n",
      "1                       Right on brah, see you later   \n",
      "2  Waiting in e car 4 my mum lor. U leh? Reach ho...   \n",
      "3  Your 2004 account for 07XXXXXXXXX shows 786 un...   \n",
      "4  Do you want a new video handset? 750 anytime a...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [that, depends, how, would, you, like, to, be,...  \n",
      "1                 [right, on, brah, see, you, later]  \n",
      "2  [waiting, in, e, car, my, mum, lor, u, leh, re...  \n",
      "3  [your, account, for, xxxxxxxxx, shows, unredee...  \n",
      "4  [do, you, want, a, new, video, handset, anytim...  \n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df_test = pd.read_csv(\"/kaggle/input/messages-data/TestData.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Apply the same preprocessing function as before\n",
    "df_test['tokens'] = df_test['message'].apply(preprocess_data)\n",
    "\n",
    "# Display first few rows\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "737ed3b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:21.083092Z",
     "iopub.status.busy": "2025-03-08T12:55:21.082736Z",
     "iopub.status.idle": "2025-03-08T12:55:21.088901Z",
     "shell.execute_reply": "2025-03-08T12:55:21.087949Z"
    },
    "papermill": {
     "duration": 0.013978,
     "end_time": "2025-03-08T12:55:21.090773",
     "exception": false,
     "start_time": "2025-03-08T12:55:21.076795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_message(tokens, P_ham, P_spam):\n",
    "    # Initialize log probabilities with priors\n",
    "    log_prob_ham = np.log(P_ham)\n",
    "    log_prob_spam = np.log(P_spam)\n",
    "\n",
    "    # Sum log probabilities of each word in the message\n",
    "    for word in tokens:\n",
    "        if word in ham_probabilities:\n",
    "            log_prob_ham += np.log(ham_probabilities[word])\n",
    "        else:\n",
    "            log_prob_ham += np.log(1 / (total_ham_words + V_size))  # Handle unseen words\n",
    "\n",
    "        if word in spam_probabilities:\n",
    "            log_prob_spam += np.log(spam_probabilities[word])\n",
    "        else:\n",
    "            log_prob_spam += np.log(1 / (total_spam_words + V_size))  # Handle unseen words\n",
    "\n",
    "    # Predict class based on higher probability\n",
    "    return \"spam\" if log_prob_spam > log_prob_ham else \"ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ee72a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:21.102346Z",
     "iopub.status.busy": "2025-03-08T12:55:21.101924Z",
     "iopub.status.idle": "2025-03-08T12:55:21.188287Z",
     "shell.execute_reply": "2025-03-08T12:55:21.187095Z"
    },
    "papermill": {
     "duration": 0.094256,
     "end_time": "2025-03-08T12:55:21.190163",
     "exception": false,
     "start_time": "2025-03-08T12:55:21.095907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "P_ham = num_ham / total_sms\n",
    "P_spam = num_spam / total_sms\n",
    "\n",
    "# Apply classification to each message\n",
    "df_test['label'] = df_test['tokens'].apply(lambda tokens: classify_message(tokens, P_ham, P_spam))\n",
    "\n",
    "# Save results to ResultData.csv\n",
    "df_test[['message', 'label']].to_csv(\"ResultData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6318621",
   "metadata": {
    "papermill": {
     "duration": 0.004704,
     "end_time": "2025-03-08T12:55:21.200279",
     "exception": false,
     "start_time": "2025-03-08T12:55:21.195575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save the Results for using Naive Bayes\n",
    "1. Output ResultData.csv with classified messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a489dd00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:21.210921Z",
     "iopub.status.busy": "2025-03-08T12:55:21.210546Z",
     "iopub.status.idle": "2025-03-08T12:55:21.223339Z",
     "shell.execute_reply": "2025-03-08T12:55:21.222120Z"
    },
    "papermill": {
     "duration": 0.020361,
     "end_time": "2025-03-08T12:55:21.225299",
     "exception": false,
     "start_time": "2025-03-08T12:55:21.204938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message label\n",
      "0  That depends. How would you like to be treated...   ham\n",
      "1                       Right on brah, see you later   ham\n",
      "2  Waiting in e car 4 my mum lor. U leh? Reach ho...   ham\n",
      "3  Your 2004 account for 07XXXXXXXXX shows 786 un...  spam\n",
      "4  Do you want a new video handset? 750 anytime a...  spam\n"
     ]
    }
   ],
   "source": [
    "# Output ResultData.csv with classified messages.\n",
    "print(pd.read_csv(\"ResultData.csv\").head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b9916",
   "metadata": {
    "papermill": {
     "duration": 0.004413,
     "end_time": "2025-03-08T12:55:21.234808",
     "exception": false,
     "start_time": "2025-03-08T12:55:21.230395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a78220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:21.245693Z",
     "iopub.status.busy": "2025-03-08T12:55:21.245340Z",
     "iopub.status.idle": "2025-03-08T12:55:22.436829Z",
     "shell.execute_reply": "2025-03-08T12:55:22.435743Z"
    },
    "papermill": {
     "duration": 1.199063,
     "end_time": "2025-03-08T12:55:22.438696",
     "exception": false,
     "start_time": "2025-03-08T12:55:21.239633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert text data to numerical features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(df_sms['message'])\n",
    "y_train = df_sms['label']\n",
    "X_test = vectorizer.transform(df_test['message'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fa0609",
   "metadata": {
    "papermill": {
     "duration": 0.00454,
     "end_time": "2025-03-08T12:55:22.448211",
     "exception": false,
     "start_time": "2025-03-08T12:55:22.443671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save the results for using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9d72a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T12:55:22.459008Z",
     "iopub.status.busy": "2025-03-08T12:55:22.458480Z",
     "iopub.status.idle": "2025-03-08T12:55:22.481323Z",
     "shell.execute_reply": "2025-03-08T12:55:22.480002Z"
    },
    "papermill": {
     "duration": 0.030473,
     "end_time": "2025-03-08T12:55:22.483300",
     "exception": false,
     "start_time": "2025-03-08T12:55:22.452827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to random_forest_results.csv\n",
      "                                             message actual_label  \\\n",
      "0  That depends. How would you like to be treated...          ham   \n",
      "1                       Right on brah, see you later          ham   \n",
      "2  Waiting in e car 4 my mum lor. U leh? Reach ho...          ham   \n",
      "3  Your 2004 account for 07XXXXXXXXX shows 786 un...         spam   \n",
      "4  Do you want a new video handset? 750 anytime a...         spam   \n",
      "\n",
      "  predicted_label  \n",
      "0             ham  \n",
      "1             ham  \n",
      "2             ham  \n",
      "3            spam  \n",
      "4            spam  \n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV\n",
    "results = pd.DataFrame({'message': df_test['message'], 'actual_label': y_test, 'predicted_label': y_pred})\n",
    "results.to_csv('random_forest_results.csv', index=False)\n",
    "print(\"Predictions saved to random_forest_results.csv\")\n",
    "\n",
    "\n",
    "# Output ResultData.csv with classified messages.\n",
    "print(pd.read_csv(\"random_forest_results.csv\").head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6723199,
     "sourceId": 10827433,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.099351,
   "end_time": "2025-03-08T12:55:23.208314",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-08T12:55:14.108963",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
